{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdNSqM1YIhqe"
   },
   "source": [
    "# Import Packages and Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 16311,
     "status": "ok",
     "timestamp": 1742853016274,
     "user": {
      "displayName": "Elvis Umana",
      "userId": "14542587579445116032"
     },
     "user_tz": 300
    },
    "id": "FjD-k9XoIhqg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rdkit-env/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Packages for modeling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C ,WhiteKernel as Wht,Matern as matk\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1742853016310,
     "user": {
      "displayName": "Elvis Umana",
      "userId": "14542587579445116032"
     },
     "user_tz": 300
    },
    "id": "MspM6ko9Ihqh"
   },
   "outputs": [],
   "source": [
    "# Uncertainty Weighting\n",
    "\n",
    "epsilon = 0 # Default = 0; Adds allowance for minimum Expected Improvement that will be recommended (additive). Increasing this will mean that EI can be lower and still recommend points which will favor exploration.\n",
    "weight = 1 # Default = 1; Weights the value of prediction uncertainty for expected improvment (multiplicative)\n",
    "\n",
    "Num_of_recs = 5 # Select desired number of recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqJIaiKlIhqi"
   },
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 3346,
     "status": "error",
     "timestamp": 1742853019665,
     "user": {
      "displayName": "Elvis Umana",
      "userId": "14542587579445116032"
     },
     "user_tz": 300
    },
    "id": "n0sMjq05Ihqi",
    "outputId": "dcb566fc-0862-4824-ebaa-3cabdde86708"
   },
   "outputs": [],
   "source": [
    "# Read in data here. Add more data below and concatenate in the next cell as needed.\n",
    "\n",
    "data_in_0 = pd.read_excel('../Data/Experimental Data/Initial.xlsx', engine='openpyxl')\n",
    "data_in_1 = pd.read_excel('../Data/Experimental Data/1st_iteration_CEs.xlsx', engine='openpyxl')\n",
    "data_in_2 = pd.read_excel('../Data/Experimental Data/2nd_iteration_CEs.xlsx', engine='openpyxl')\n",
    "data_in_3 = pd.read_excel('../Data/Experimental Data/3rd_iteration_CE.xlsx', engine='openpyxl')\n",
    "data_in_4 = pd.read_excel('../Data/Experimental Data/4th_iteration_CE.xlsx', engine='openpyxl')\n",
    "data_in_5 = pd.read_excel('../Data/Experimental Data/5th_iteration_CE.xlsx', engine='openpyxl')\n",
    "data_in_6 = pd.read_excel('../Data/Experimental Data/6th_iteration_CE.xlsx', engine='openpyxl')\n",
    "#data_in_7 = pd.read_excel('../Data/Experimental Data/7th_iteration_CE.xlsx', engine='openpyxl')\n",
    "\n",
    "#all_exptl_data = pd.read_excel('Data/exptl_data_241217.xlsx', engine='openpyxl')\n",
    "#print(data_in_alltest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QoiCbvGaIhqi",
    "outputId": "aa16f7a5-4c42-4ac8-f55c-73015b0c6a8b"
   },
   "outputs": [],
   "source": [
    "Gaus_data_0 = data_in_0[['#', '<CE> (%)', 'DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']].dropna() # Drops any incomplete data in these columns\n",
    "#Gaus_data = Gaus_data_0 # Comment this out if you have more than 1 input file\n",
    "Gaus_data_1 = data_in_1[['#', '<CE> (%)', 'DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']].dropna() # Uncomment to add more data.\n",
    "Gaus_data_2 = data_in_2[['#', '<CE> (%)', 'DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']].dropna()\n",
    "Gaus_data_3 = data_in_3[['#', '<CE> (%)', 'DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']].dropna()\n",
    "Gaus_data_4 = data_in_4[['#', '<CE> (%)', 'DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']].dropna()\n",
    "Gaus_data_5 = data_in_5[['#', '<CE> (%)', 'DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']].dropna()\n",
    "Gaus_data_6 = data_in_6[['#', '<CE> (%)', 'DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']].dropna()\n",
    "#Gaus_data_7 = data_in_7[['#', '<CE> (%)', 'DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']].dropna()\n",
    "\n",
    "#Gaus_data = pd.concat((Gaus_data_0, Gaus_data_1)).reset_index(drop=True) # Uncomment to combine mulitple data files into 1\n",
    "Gaus_data = pd.concat((Gaus_data_0, Gaus_data_1, Gaus_data_2, Gaus_data_3, Gaus_data_4, Gaus_data_5, Gaus_data_6)).reset_index(drop=True) # Uncomment to combine mulitple data files into 1\n",
    "\n",
    "# print(Gaus_data) # Uncomment to see data\n",
    "\n",
    "Gaus_data['<CE> (%)'] = np.log10(np.reciprocal(1 - (Gaus_data['<CE> (%)']/100))) # Log scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pKCMjvajIhqj"
   },
   "outputs": [],
   "source": [
    "# Read in Data Space\n",
    "\n",
    "Plotting_points = pd.read_excel('../Data/Data Space/0.05 Mole Interval.xlsx', engine='openpyxl', index_col=0) # Read in by 0.05 for plotting\n",
    "Rec_points = pd.read_excel('../Data/Data Space/0.1 Mole Interval.xlsx', engine='openpyxl', index_col=0) # Read in by 0.1 for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lvscRiBiIhqj"
   },
   "outputs": [],
   "source": [
    "Y_scale = StandardScaler()\n",
    "X_scale = StandardScaler()\n",
    "\n",
    "Plotting_points_scaled = X_scale.fit_transform(Plotting_points) # Scale data space for GP modeling\n",
    "Rec_points_scaled = X_scale.transform(Rec_points) # Scale data space for GP modeling\n",
    "\n",
    "\n",
    "X = X_scale.transform(Gaus_data[['DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']]) # Collected data transformed for GP modeling\n",
    "X_unscaled = Gaus_data[['DME', 'DEGDME', 'TEGDME', 'CH3DME', 'THF', 'MTHF', 'THP', 'diethyl ether', 'DEE', 'DOL']] # Collected data without CE %\n",
    "\n",
    "Y = Y_scale.fit_transform(Gaus_data[['<CE> (%)']]) # Scale for GP regression target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHIFn9X5Ihqk"
   },
   "source": [
    "# GP Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9tQePHAzIhqk",
    "outputId": "fa1af98a-4b17-41dd-e5d4-75960e50f9e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rdkit-env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified upper bound 100000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/rdkit-env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 100000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/rdkit-env/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Kernel = 1 * Matern(length_scale=1, length_scale_bounds=(0.1, 2), nu=1.5) + Wht(1.0, (1e-6, 1000))\n",
    "kernel_lit = C(1.0, (1e-3,1e3)) * matk([1.0]*10,[[1e-12, 1e8]]*10,1.5) + Wht(1.0, (1e-6, 1e3))\n",
    "Gaus_model = GaussianProcessRegressor(kernel=kernel_lit, n_restarts_optimizer=30, random_state=15)\n",
    "Gaus_model.fit(X,Y)\n",
    "\n",
    "Pred, Std = Gaus_model.predict(Plotting_points_scaled, return_std = True)\n",
    "Pred_Next, Std_Next = Gaus_model.predict(Rec_points_scaled, return_std = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rff9rYVbIhqk",
    "outputId": "dcebc82a-d567-485a-f835-b5b92f4da96c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rdkit-env/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Pred_x, Std_x = Gaus_model.predict(X, return_std = True)\n",
    "ybest = Y_scale.transform(np.array(Gaus_data['<CE> (%)']).max().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2S9ScG91Ihqk"
   },
   "outputs": [],
   "source": [
    "zzval=((Pred-ybest-epsilon)/Std)\n",
    "expI=(Pred-ybest-epsilon)*norm.cdf(zzval)+Std*norm.pdf(zzval)*weight\n",
    "\n",
    "for i in range(0,expI.shape[0]):\n",
    "    if Std[i] < 0:\n",
    "        expI[i] = 0\n",
    "\n",
    "zzval_Next=((Pred_Next-ybest)/Std_Next)\n",
    "expI_Next=(Pred_Next-ybest-epsilon)*norm.cdf(zzval_Next)+Std_Next*norm.pdf(zzval_Next)*weight\n",
    "\n",
    "for i in range(0,expI_Next.shape[0]):\n",
    "    if Std_Next[i] < 0:\n",
    "        expI_Next[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-VymaqlVIhqk",
    "outputId": "537860d0-9bd5-4939-ca0b-1104c038400c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "# Copying data, predicting, and creating a sorted dataframe for plotting points\n",
    "Plotting_points_pred = Plotting_points.copy()\n",
    "Plotting_points_pred['Predicted CE'], Plotting_points_pred['Log Predicted CE StD'], Plotting_points_pred['Expected Improvement'] = (1-np.reciprocal(10**Y_scale.inverse_transform(Pred.reshape(-1, 1))))*100, Y_scale.inverse_transform(Std.reshape(-1, 1)) - Y_scale.mean_[0], expI.reshape(-1,1)\n",
    "Plotting_points_pred_sorted = Plotting_points_pred.copy().sort_values(by='Expected Improvement', ascending=True)\n",
    "\n",
    "# Copying data, predicting, and creating a sorted dataframe for recommending points\n",
    "Rec_points_pred = Rec_points.copy()\n",
    "Rec_points_pred['Predicted CE'], Rec_points_pred['Log Predicted CE StD'], Rec_points_pred['Expected Improvement'] = (1-np.reciprocal(10**Y_scale.inverse_transform(Pred_Next.reshape(-1, 1))))*100, Y_scale.inverse_transform(Std_Next.reshape(-1, 1)) - Y_scale.mean_[0], expI_Next.reshape(-1,1)\n",
    "Rec_points_pred_sorted = Rec_points_pred.copy().sort_values(by='Expected Improvement', ascending=True)\n",
    "\n",
    "# Creating dataset for farthest point sampling if needed\n",
    "FPS_Select_Scaled = pd.DataFrame(Rec_points_scaled, columns=Rec_points.columns) # Farthest point sampling in case it is needed later\n",
    "FPS_Select_Scaled['Predicted CE'], FPS_Select_Scaled['Log Predicted CE StD'], FPS_Select_Scaled['Expected Improvement'] = Y_scale.inverse_transform(Pred_Next.reshape(-1, 1)), Y_scale.inverse_transform(Std_Next.reshape(-1, 1)) - Y_scale.mean_[0], expI_Next.reshape(-1,1)\n",
    "\n",
    "# Check how many points are tied for the maximum EI and create data frame\n",
    "Maximum_EI_df = FPS_Select_Scaled.loc[np.round(Rec_points_pred['Expected Improvement'],3) == np.round(Rec_points_pred['Expected Improvement'].max(), 3)].copy().drop(['Predicted CE', 'Log Predicted CE StD', 'Expected Improvement'], axis=1).reset_index(drop=True)\n",
    "Maximum_EI_df_full = Rec_points_pred.loc[np.round(Rec_points_pred['Expected Improvement'],3) == np.round(Rec_points_pred['Expected Improvement'].max(), 3)].copy().reset_index(drop=True)\n",
    "\n",
    "# Print how many data points are tied for the maximum\n",
    "print(Maximum_EI_df.shape[0], Maximum_EI_df_full.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hKqTpEXgIhql"
   },
   "outputs": [],
   "source": [
    "if Maximum_EI_df.shape[0] > Num_of_recs:\n",
    "\n",
    "    FPS_points_full = Rec_points_pred.loc[Rec_points_pred['Expected Improvement'] == Rec_points_pred['Expected Improvement'].max()].copy().iloc[0:1]\n",
    "\n",
    "    FPS_points = pd.DataFrame(X, columns=Plotting_points.columns)\n",
    "\n",
    "    for i in tqdm(range(0,Num_of_recs)):\n",
    "        distances = np.zeros(Maximum_EI_df.shape[0])\n",
    "        for j in range(0,Maximum_EI_df.shape[0]):\n",
    "            temp = Maximum_EI_df.iloc[j].to_numpy().reshape(1,10) - FPS_points\n",
    "            distances[j] = np.min(np.sum(np.square(temp), axis=1))\n",
    "        max_idx = np.argmax(distances)\n",
    "        FPS_points = pd.concat((FPS_points, Maximum_EI_df.iloc[max_idx:max_idx+1]), axis=0).reset_index(drop=True)\n",
    "\n",
    "        FPS_points_full = pd.concat((FPS_points_full, Maximum_EI_df_full.iloc[max_idx:max_idx+1]), axis=0).reset_index(drop=True)\n",
    "\n",
    "    Recs = FPS_points_full.reset_index(drop=True).iloc[-Num_of_recs:].sort_values(by='Expected Improvement', ascending=False).reset_index(drop=True)\n",
    "\n",
    "else:\n",
    "    Recs = Rec_points_pred_sorted.iloc[-Num_of_recs:].sort_values(by='Expected Improvement', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "A5WIstONIhql",
    "outputId": "8bd6d077-3e1b-4930-d752-4564d0b4070e"
   },
   "outputs": [],
   "source": [
    "# Export Recs\n",
    "Recs.to_excel('../Data/Recs/Recs - ' + 'epsilon ' + str(epsilon) + ' weight ' + str(weight) + '.xlsx')\n",
    "# print(Recs) # Uncomment to see recommended solvents\n",
    "\n",
    "#export all points:\n",
    "Rec_points_pred_sorted.to_excel('../Data/Recs/all_predictions_EIsorted.xlsx')\n",
    "Rec_points_pred.to_excel('../Data/Recs/all_predictions_byNumber.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9HQ872rAIhql",
    "outputId": "e3d9f64e-c752-40de-e8cf-16092848f3bd"
   },
   "outputs": [],
   "source": [
    "# Generate top 3 predictions:\n",
    "\n",
    "#all points sorted by CE, equivalent to Rec_points_pred_sorted\n",
    "CE_sortlist = Rec_points_pred.copy().sort_values(by='Predicted CE', ascending=True)\n",
    "\n",
    "#df of scaled parameter space for the compositions with max CE,\n",
    "Maximum_CE_df_full = Rec_points_pred.loc[np.round(Rec_points_pred['Predicted CE'],3) == np.round(Rec_points_pred['Predicted CE'].max(), 3)].copy().reset_index(drop=True)\n",
    "\n",
    "#above df with predictions dropped\n",
    "Maximum_CE_df = Maximum_CE_df_full.copy().drop(['Predicted CE', 'Log Predicted CE StD', 'Expected Improvement'], axis=1).reset_index(drop=True)\n",
    "\n",
    "if Maximum_CE_df.shape[0] > 3: #if there are more than 3 top entries, proceed with FPS\n",
    "\n",
    "    FPS_points_full = Rec_points_pred.loc[Rec_points_pred['Predicted CE'] == Rec_points_pred['Predicted CE'].max()].copy().iloc[0:1]\n",
    "\n",
    "    FPS_points = pd.DataFrame(X, columns=Plotting_points.columns)\n",
    "\n",
    "    for i in tqdm(range(0,3)):\n",
    "        distances = np.zeros(Maximum_CE_df.shape[0])\n",
    "        for j in range(0,Maximum_CE_df.shape[0]):\n",
    "            temp = Maximum_CE_df.iloc[j].to_numpy().reshape(1,10) - FPS_points\n",
    "            distances[j] = np.min(np.sum(np.square(temp), axis=1))\n",
    "        max_idx = np.argmax(distances)\n",
    "        FPS_points = pd.concat((FPS_points, Maximum_CE_df.iloc[max_idx:max_idx+1]), axis=0).reset_index(drop=True)\n",
    "\n",
    "        FPS_points_full = pd.concat((FPS_points_full, Maximum_CE_df_full.iloc[max_idx:max_idx+1]), axis=0).reset_index(drop=True)\n",
    "\n",
    "    top3Recs = FPS_points_full.reset_index(drop=True).iloc[-3:].sort_values(by='Predicted CE', ascending=False).reset_index(drop=True)\n",
    "\n",
    "else:\n",
    "    top3Recs = CE_sortlist.iloc[-3:].sort_values(by='Predicted CE', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# print(top3Recs) Uncomment to see top 3 recommendations by CE\n",
    "\n",
    "# Export Recs\n",
    "top3Recs.to_excel('../Data/Recs/top3.xlsx')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
